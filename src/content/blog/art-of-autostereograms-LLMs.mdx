---
title: LLMs and the Art of Autostereograms.
description: Tailwind CSS is a highly customizable, utility-first CSS framework that streamlines the process of designing and building user interfaces.
pubDate: 2023-10-16
updatedDate: 2023-10-16
hero: "~/assets/heros/magic-eye-books.png"
heroAlt: "Book covers with Magic Eye Auto stereograms Part 1, 2, 3 "
tags:
  [
    "Auto stereogram",
    "AI Art",
    "LLM",
    "Large Language Models",
    "Pattern Generation",
    "depth-maps",
  ]
---

import ImageList from "./ImageList.tsx";


# Problem Domain and Brief Project Description

Stereograms are two-dimensional images that, when viewed correctly, create the illusion of three-dimensional depth. This effect can be achieved using various techniques, including presenting slightly different images to each eye or employing patterns that manipulate depth perception through binocular disparity. Autostereograms, a specific type of stereogram, allow viewers to perceive 3D shapes from a single image without any special equipment.

Incorporating AI and large language models (LLMs) into the generation of stereograms enhances this process by allowing for the extraction and integration of essential visual features, such as colors and shapes, from the original images. This not only improves the visual quality of the generated stereograms but also helps preserve the artistic elements that are often lost in traditional methods. By leveraging AI, I aim to create a more engaging and meaningful viewing experience that honors the original artwork while providing depth perception through autostereograms.

# More Detailed Description of the Approach

The creation of autostereograms can be approached in two main ways: manual pattern generation and AI-driven pattern generation using Large Language Models (LLMs). While both methods utilize the features of a source image, they do so through different techniques, offering a wide array of creative possibilities. But here I have followed a manual approach to extract important features and fed the features to AI to get results that not only have features from original image but also the Creativity of AI image generators

## AI-Based Pattern Generation
  <ImageList client:load  images={[{
    src: "https://miro.medium.com/v2/resize:fit:968/format:webp/0*eE4XwsUqTTmWOGKW",
    alt: "Flow of the pattern generation process",
    height: 500,
  }]} />

### Step 1: Image Upload

The workflow begins with users uploading their images. This simple action sets the stage for the creative transformation that follows.

### Step 2: Extracting Features

Once an image is uploaded, the next step is to extract its essential visual features, focusing on dominant colors and shapes through two key functions: `extract_color` and `extract_shapes`.

#### Extracting Dominant Colors

The `extract_color` function uses k-means clustering to identify the top three dominant colors in the image:

1.  Convert to RGB: The image is converted into a NumPy array in RGB format.
2.  Reshape for Clustering: The image is reshaped into a 2D array of pixel values.
3.  K-Means Clustering: The k-means algorithm groups similar colors into clusters.
4.  Extract Colors: The RGB values of the cluster centers are retrieved as the dominant colors.
5.  Convert to Hex Codes: These RGB values are then converted into hexadecimal color codes for easy use.

The result is a list of hex codes representing the dominant colors.

```py
def extract_color(img):
    """Extract dominant colors from the image using k-means clustering and return their hex codes."""
    img_rgb = np.array(img)
    # Reshaping the image to a 2D array of pixels
    pixels = img_rgb.reshape((-1, 3))
    # Apply k-means clustering to find dominant colors
    kmeans = KMeans(n_clusters=3)
    kmeans.fit(pixels)
    # Get the dominant colors
    dominant_colors = kmeans.cluster_centers_.astype(int)
    # Convert RGB to hex
    hex_colors = [        "#{:02x}{:02x}{:02x}".format(color[0], color[1], color[2])
        for color in dominant_colors
    ]
    return hex_colors
```

#### Extracting Shapes

The `extract_shapes` function identifies and counts various shapes in the image:

1.  Grayscale Conversion: The image is converted to grayscale to simplify processing.
2.  Edge Detection: The Canny algorithm detects edges in the blurred image.
3.  Find Contours: Contours are extracted to outline shapes.
4.  Shape Classification: Each contour is analyzed by its number of vertices to classify it as a Triangle, Rectangle, Square, or Circle.

```py
def extract_shapes(img):
    """Extract shapes from the image and list them."""
    gray = np.array(img.convert("L"))
    # Apply edge detection using Canny
    edges = cv2.Canny(blurred, 50, 150)
    # Find contours
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    shapes = {"Triangle": 0, "Rectangle": 0, "Square": 0, "Circle": 0}
    for contour in contours:
        epsilon = 0.04 * cv2.arcLength(contour, True)
        approx = cv2.approxPolyDP(contour, epsilon, True)
        num_vertices = len(approx)
        if num_vertices == 3:
            shapes["Triangle"] += 1
        elif num_vertices == 4:
            x, y, w, h = cv2.boundingRect(approx)
            aspect_ratio = float(w) / h
            if 0.95 < aspect_ratio < 1.05:
                shapes["Square"] += 1
            else:
                shapes["Rectangle"] += 1
        elif num_vertices > 4:
            shapes["Circle"] += 1
    return shapes
```

The output is a dictionary counting each shape type found in the image.Together, these functions capture both color and shape information, providing valuable features for subsequent steps in the workflow.

### Step 3: Generating a Prompt

In this step, we create a prompt for an AI model to generate a tile pattern based on the extracted dominant colors and shapes. The `generate_prompt` function performs the following tasks:

1.  **Constructing the Prompt**: It formats a pre-existing prompt with the colors and shapes extracted from the input image

```py
prompt = f"""Convert the following hex colors to human-readable colors: {
', '.join(dominant_colors)}. Identify the most occurring shapes from the list:
{', '.join([f'{k}: {v}' for k, v in shapes.items()])}.
Using this data,
create an image generator prompt for a tile pattern that will be repeated.
Incorporate the identified colors and shapes, including circular patterns.
Blend the shapes seamlessly into the pattern,
prioritizing the most frequent shapes and colors.
Please provide the final image generator prompt only.
"""
```

##### Why did the prompt ask for human readable color names?

Human-readable color names work better in text-to-image generation because the model is trained to understand colors by their names (like “red” or “blue”) rather than specific codes. Hex codes can confuse the model and lead to unexpected results, like using the wrong shade. Color names help the model generate more accurate and consistent images since it knows how these colors typically look and how they’re used in different contexts. Using names reduces errors and makes the results more predictable, ensuring the color matches what the user expects.

For text-to-image models using Flux (or similar frameworks), training data like **COCO**, **Visual Genome**, and **OpenImages** are commonly used. These datasets contain images with annotated captions, including color names, which help the model associate textual descriptions with visual features. By learning from such datasets, the model can generate more accurate and consistent images, interpreting color names as expected.

2. **Incorporating Details**: The prompt instructs the **google/gemma-7b-it-lora** model to create an image generator prompt for a tile pattern, emphasizing circular patterns and seamless blending of shapes, while prioritizing the most frequent colors and shapes.

This process effectively transforms our extracted features into a prompt for creating visually appealing tile patterns that reflect the original image’s characteristics.

### Step 4: Creating a Patterned Image

The refined prompt generated by Gemma-7B-IT-LoRA is in the previous step is then fed into the Black Forest Labs’ Flux-1-Schnell model. This model utilizes the information from the original input image to produce a unique patterned image that reflects the identified features.

```py
def generate_pattern_from_prompt(prompt):
    try:
        data = {
            "prompt": prompt,
        }
        response = requests.post(
            f"https://api.cloudflare.com/client/v4/accounts/{CLOUDFLARE_ACCOUNT_ID}/ai/run/@cf/black-forest-labs/flux-1-schnell",
            headers={"Authorization": f"Bearer {CLOUDFLARE_API_TOKEN}"},
            json=data,
            timeout=10,
        )
        if response.status_code == 200:
            data = response.json()
            base64_image = data.get("result", {}).get("image", None)
            if base64_image is None:
                print("Error: No 'image' field in response data")
                return None
            image_data = base64.b64decode(base64_image)
            image = Image.open(io.BytesIO(image_data))
            image = image.resize((384, 384))
            return image
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        print(f"Error generating pattern from prompt using Cloudflare AI: {e}")
        return None
```

### Step:5 Depth Map Generation

Once the pattern is created, the next crucial step is generating the **depth map**, which encodes the 3D structure within the 2D pattern. This depth map is generated using one of three popular depth estimation models:

- **MiDaS/Small**: A lighter, faster model suitable for quicker depth map generation.
- **DPT/Large**: A more detailed model that provides high accuracy but may take more time to process.
- **DPT/Hybrid**: A hybrid model that balances speed and accuracy, ideal for most general use cases.

```py
def estimate_depth(image, model, transform):
    original_size = image.size
    image = image.resize((384, 384))
    input_batch = transform(image).unsqueeze(0)
    with torch.no_grad():
        depth_map = model(input_batch).squeeze().cpu().numpy()
    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())
    depth_map = (depth_map * 255).astype(np.uint8)
    depth_map = cv2.resize(depth_map, original_size, interpolation=cv2.INTER_LINEAR)
    return depth_map
```

Users can choose between these models based on their specific needs — whether they prioritize speed, accuracy, or a balance of both. The generated depth map is then applied to the pattern, allowing the hidden 3D shape to emerge when the image is viewed correctly. ( in most cases DPT_Large works perfectly). The app is hosted on streamlit [(here)](https://rapo7-stereogram-pattern-generator-app-eppbyy.streamlit.app/).

### Step 6: Autostereogram Creation

Now that we have the pattern and the depth_map now we have to generate the auto stereogram.

Adaptive Stripes Algorithm:

#### 1. Load Input Data

- **Depth Map**: Load the depth map image (`depth_map_path`), which should be in grayscale format where lighter areas represent closer points and darker areas represent farther points.
- **Base Pattern**: Load the base pattern image (`base_pattern_path`), which will be repeated to form the autostereogram.

#### 2. Initialize Parameters

- **Image Dimensions**:
- Let `height` be the height of the depth map.
- Let `width` be the width of the depth map.
- Let `base_pattern_width` be the width of the base pattern (for tiling purposes).
- **Stripe Width (P)**: This is the horizontal width of each vertical stripe in the autostereogram.

#### 3. Create an Empty Output Image

- Initialize an output image (`output_image`) with the same height as the depth map and the same width as the target image.
- Set the output image’s pixel values to 0 (black), to be updated later.

4. Apply the Base Pattern to the First Stripe

- **First Stripe**: For the first `stripe_width` columns of each row:
- Copy the corresponding pixel from the base pattern (`base_pattern`) to the output image, wrapping around vertically and horizontally to ensure tiling.

#### 5. Apply Depth-Driven Displacement

- For each pixel in the rest of the image (from `stripe_width` to `width`):
- Calculate the **displacement** for the pixel based on the depth map value at that pixel:

```py
displacement = depth_map(y,x) / 10
​
```

- The displacement controls how far the pixel will be moved horizontally.
- Adjust the pixel’s horizontal position based on the displacement, wrapping around if necessary:

```py
new_x = (x−stripe_width+displacement) mod stripe_width
```

Copy the pixel from the first stripe to the new position in the output image.

#### 6. Display the Output Image

- Convert the final `output_image` array into an image format (e.g., PNG, JPEG) and save it to the specified `output_path`.

```py
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
def generate_autostereogram(depth_map, base_pattern, P, k):
    """
    Generates an autostereogram based on a depth map and base pattern.

    Parameters:
    - depth_map (np.array): The depth map as a grayscale image (H x W).
    - base_pattern (np.array): The base pattern as a grayscale image (H x W_base).
    - P (int): The repetition period, i.e., the width of each stripe.
    - k (float): Displacement factor that scales the depth values to horizontal shifts.

    Returns:
    - np.array: The generated autostereogram (H x W).
    """
    H, W = depth_map.shape  # Height and width of the depth map
    W_base = base_pattern.shape[1]  # Width of the base pattern

    # Initialize the target autostereogram image
    autostereogram = np.zeros((H, W), dtype=np.uint8)

    # Number of stripes
    num_stripes = W // P

    for i in range(num_stripes):
        for y in range(H):
            for x in range(P):
                # Get the depth value for the current pixel (y, i * P + x)
                depth_value = depth_map[y, i * P + x]

                # Calculate the horizontal displacement based on depth value
                displacement = depth_value * k

                # Calculate the new x-coordinate in the base pattern (wrap around)
                new_x = int((x - displacement) % W_base)

                # Copy the corresponding pixel from the base pattern to the autostereogram
                autostereogram[y, i * P + x] = base_pattern[y, new_x]

    return autostereogram
# Load the depth map and base pattern
depth_map = np.array(Image.open("depth_map.png").convert("L"))  # Convert depth map to grayscale (L mode)
base_pattern = np.array(Image.open("base_pattern.png").convert("L"))  # Convert base pattern to grayscale
# Parameters
P = 10  # Width of each stripe
k = 5   # Displacement factor (tune this based on desired depth effect)
# Generate the autostereogram
autostereogram = generate_autostereogram(depth_map, base_pattern, P, k)
# Display the resulting autostereogram
plt.imshow(autostereogram, cmap="gray")
plt.axis("off")  # Hide axes
plt.show()
```

The algorithm is heavily inspired by the below repository
[https://github.com/piellardj/stereogram-webgl](https://github.com/piellardj/stereogram-webgl)

I have made a web app that allows us to customize the number of vertical stripes and depth map and pattern.

# Results and Evaluation

I myself have high astigmatism so I cant see stereograms at all. so I have taken help of an online stereogram detector to detect autostereograms.

As I have mentioned heavily about art.. lets start with mona lIsa by Leonardo Da vinci.

## Mona Lisa by Leonardo da Vinci.

3 colors detected were #a19557 (Olive drab) #251722 (Maroon) #625538 (Sea green). and shapes are Triangle: 3 Rectangle: 41 Square: 0 Circle: 42:
The pattern is pretty accurate on this one and color names in brackets are detected by gemma LLM they are pretty accurate too. and we can find more circles and rectangles in the pattern. The revealed stereogram is good witout any extra noise .. I have divided the image into 6 vertical sections for repeating on this one.

<ImageList client:load  images={[{
    src: "https://miro.medium.com/v2/resize:fit:574/format:webp/1*T8PbjILkSx4uYiAeR9p_IQ.jpeg",
    alt: "Mona Lisa"
}, 
{
    src: "https://miro.medium.com/v2/resize:fit:768/format:webp/1*1-TQjcaw_ndO3TjYoJUgHw.jpeg",
    alt: "Pattern with dominant colors"
},
{
 src: "https://miro.medium.com/v2/resize:fit:574/format:webp/1*Ga-11-3ED1Nq8eDuoAxWGQ.jpeg",
    alt: "Depth Map of the Mona Lisa using DPT_Large"
}
]} />

---

<ImageList client:load  images={[{
    src: "https://miro.medium.com/v2/resize:fit:574/format:webp/1*kVIPpQkiMpayBxTICBEbRg.png",
    alt: "Generated stereogram"
},
{
    src: "https://miro.medium.com/v2/resize:fit:1024/format:webp/1*WOXP2VTKBZHUMAbFLv7Qyw.png",
    alt: "Revealed stereogram from an online generator"
}
]} />


## The Son of Man by René Magritte

Now lets use the **_The Son of Man by_** René Magritte The image was detected to have #c6c4b6 (Light Gray) #393634 (Dark Gray) #8f9789 (Light Green)and shapes were Triangle: 1 Rectangle: 7 Square: 0 Circle: 11so resulting patern has these colros and recatngles and circles .. **in most of the artistic images the circle and recatngle count seemed very high**. so all patterns from here followed the same.

<ImageList client:load  images={[{
    src: "https://miro.medium.com/v2/resize:fit:532/format:webp/1*d3k4DjWhz529JIY0twjilA.jpeg",
    alt: "The Son of Man by René Magritte"},
{
    src: "https://miro.medium.com/v2/resize:fit:768/format:webp/1*CETPA3btlcWAmeAGtb7Jeg.jpeg",
    alt: "Pattern with dominant colors"
},
{
    src: "https://miro.medium.com/v2/resize:fit:532/format:webp/1*hJuCy6j9l1LJ7x1idtyWyw.jpeg",
    alt: "Depth Map of The Son of Man using DPT_Large"
}
    
    
    ]} />
<ImageList client:load  images={[{
    src: "https://miro.medium.com/v2/resize:fit:636/format:webp/1*dO88jfXfrXI0xVv_C-5_6A.png",
    alt: "Generated stereogram"
},
{
    src: "https://miro.medium.com/v2/resize:fit:634/format:webp/1*yXFeCR0QVBMq8hZgosS3rQ.png",
    alt: "Revealed stereogram from an online generator"
},
{
    src: "https://miro.medium.com/v2/resize:fit:734/format:webp/1*w9cPjY2d7JN8upjpE2krSA.png",
    alt: "Interesting finding"
}
]} />


In the above image. As i used 8 vertical stripes for the image there are 8 shadows of the son of the man. I found this interesting and might be more interesting to people who can see autostereograms.

## Girl with a Pearl Earring by Johannes Vermeer

The third image has a depth map that is a little bit off when generated with DPT_large and somewhat acceptable output is produces by DPT_Hybrid. The colors detected are #110d13 (deep gray) #c1ae9d (pastel pink) #665449 (dark brown). I assume the pink is from the face of the girl. but the interesting thing here is when I used 10 repititions on the image. I was able to find a perfect subject outline in one of the overlaps eventhough the depth map is a little cloudy behind the girl.

<ImageList client:load images={[
  {
    src: "https://miro.medium.com/v2/resize:fit:518/format:webp/1*yNEwMr2EhU7AFD-gnImt5A.jpeg",
    alt: "Girl with a Pearl Earring by Johannes Vermeer"
  },
  {
    src: "https://miro.medium.com/v2/resize:fit:546/format:webp/1*scsG27n_o9983xv2M0n4FA.jpeg",
    alt: "Pattern with dominant colors"
  },
  {
    src: "https://miro.medium.com/v2/resize:fit:518/format:webp/1*TsVswsROpemNdiYFrDNr4w.jpeg",
    alt: "Depthmap using DPT_Hybrid"
  }
]} />
<ImageList client:load images={[
  {
    src: "https://miro.medium.com/v2/resize:fit:678/format:webp/1*zAiI8OPIZNqhU1_hKhmQRw.png",
    alt: "Generated stereogram"
  },
  {
    src: "https://miro.medium.com/v2/resize:fit:642/format:webp/1*2z78g7hiFoB-bMsJ2Grm2g.png",
    alt: "Revealed stereogram from an online generator"
  },
    {
        src: "https://miro.medium.com/v2/resize:fit:682/format:webp/1*2tmLL9Ppo5gGzKluKnHL2A.png",
        alt: "Interesting finding"
    }
]} />

## American Gothic by Grant Wood

The depth map is on point with dpt_large and the colors found are also good and LLM managed to do a good job in getting the names right #242019 (Deep Brown) #c1be98 (Light Tan) #77694e (Mid-Brown)..But why is the stereogram gotten worse. because I have used 16 stripes to generate this stereogram and that had messed up the detectiion. I think people viewing the autostereogram must find it hard and their eyes have to be more corssed.


<ImageList client:load images={[
  {
    src: "https://miro.medium.com/v2/resize:fit:624/format:webp/1*N6xB8RJb4kCvUJ_QJYoOiA.jpeg",
    alt: "American Gothic by Grant Wood"
  },
  {
    src: "https://miro.medium.com/v2/resize:fit:754/format:webp/1*gcQ2CwxeT1Y0IAaVA1u1MA.jpeg",
    alt: "Pattern with dominant colors"
  },
  {
    src: "https://miro.medium.com/v2/resize:fit:624/format:webp/1*ItTYoTImsApMUgRzZp-Zgw.jpeg",
    alt: "Depthmap using DPT_Large"
  }
]} />
<ImageList client:load images={[
  {
    src: "https://miro.medium.com/v2/resize:fit:996/format:webp/1*FjW6FmaU4E1OhXXWgukrPA.png",
    alt: "Generated stereogram"
  },
  {
    src: "https://miro.medium.com/v2/resize:fit:1006/format:webp/1*8YaR35YtMZZRAFMNaFVQZw.png",
    alt: "Revealed stereogram from an online generator"
  }
]} />

Overall The approach does well on Artistic images but the as we increase number of stripes in the stereogram it is harder and harder to detect the autostereogram. and all the depthmaps have only one subject except for the last one its two people but considered to be one subject. Tried on other kinds of images ( feel free to do so using website links below). the color detectioon is good but the LLM is not able to convert the colors to human readable form all colors don’t have names.

# Advantages of the LLM-Based Approach

**Adaptability to Viewer Preferences:** The system can be tailored to generate different types of patterns based on user preferences, such as varying the number of stripes or selecting different depth estimation models. This adaptability ensures that users can optimize their experience based on their specific needs and viewing conditions.

**Creative Possibilities:** The combination of manual feature extraction with AI-driven pattern generation allows for a broader range of creative outputs. This hybrid approach enables the incorporation of unique artistic styles and patterns that reflect the characteristics of the original image while also introducing innovative design elements

# Limitations and Areas for Improvement

While the system showed promise, there are still several limitations that need to be addressed:

- **Precision of Depth Maps**: For more complex or highly detailed scenes, the depth maps generated by the models (especially the MidAS — Small) sometimes lacked the precision needed for a fully effective 3D illusion. This could result in subtle misalignments or distortions in the depth effect, which hindered the clarity of the 3D object in the final image.
- **Pattern Generation Artifacts**: While the AI-generated patterns offer a more intricate and dynamic result, they can sometimes introduce visual artifacts that make the depth illusion less clear. These artifacts often arise due to the AI’s tendency to introduce randomness in ways that don’t always align with the depth cues necessary for a proper autostereogram.
- **Color detection**: This approach is well suited for art because most of the colors in historic art have a name. When tried with other images whose colors are not named yet or have one subject but wide variety of colors the LLM fails to name the colors correctly and hence generates wrong prompt resulting in pattern thats no where near original image.
- **Viewer Perception**: The system, particularly in its current form, still faces challenges in ensuring that the generated autostereograms are universally perceptible under typical viewing conditions. The effectiveness of the depth illusion can vary depending on factors such as the viewer’s ability to focus, the clarity of the depth map, and the complexity of the pattern.

I am a web developer so I built two websites

**For pattern generationn and Depth_map generation:**

[https://rapo7-stereogram-pattern-generator-app-eppbyy.streamlit.app/](https://rapo7-stereogram-pattern-generator-app-eppbyy.streamlit.app/)

**For stereogram generation :** [https://rapo7.github.io/stereogram-generator/](https://rapo7.github.io/stereogram-generator/)
